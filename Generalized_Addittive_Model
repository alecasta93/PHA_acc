#!/usr/bin/env python
# coding: utf-8
"""
Compositional analysis of VFA data for PHBV and HB:HV production.

Author : <Alessio Castagnoli>
Created: 2025-07-10
Python : ≥3.9
"""

# =======================================================================
# 0 · Imports and global settings
# =======================================================================

from __future__ import annotations

import io
import random
from pathlib import Path

import contextlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.ticker as mticker
from scipy.optimize import minimize
import statsmodels.api as sm
from pygam import LinearGAM, s

# Reproducibility --------------------------------------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

plt.rcParams.update(
    {
        "font.family": "serif",
        "axes.labelsize": 11,
        "axes.titlesize": 12,
        "figure.dpi": 150,
    }
)

# =======================================================================
# 1 · Utility functions
# =======================================================================


def clr_transform(df: pd.DataFrame, eps: float = 1e-12) -> pd.DataFrame:
    """Centered log-ratio transformation (Aitchison, 1986)."""
    arr = np.maximum(df.to_numpy(), eps)
    gmean = np.exp(np.mean(np.log(arr), axis=1, keepdims=True))
    clr = np.log(arr / gmean)
    return pd.DataFrame(clr, columns=df.columns, index=df.index)


def single_clr(arr: np.ndarray, eps: float = 1e-12) -> np.ndarray:
    """CLR for a single 1×4 vector."""
    arr = np.maximum(arr, eps)
    gmean = np.exp(np.mean(np.log(arr)))
    return np.log(arr / gmean)


def fit_ols_constrained(X: pd.DataFrame, y: pd.Series) -> sm.regression.linear_model.RegressionResultsWrapper:
    """OLS with sum-to-zero constraint."""
    Xc = sm.add_constant(X)
    try:
        return sm.OLS(y, Xc).fit_constrained(" + ".join(X.columns) + " = 0")
    except AttributeError:
        # Reparameterisation fallback: clr variables minus clr_But
        Xr = pd.DataFrame(
            {
                "const": 1.0,
                "X1": X["clr_Ac"] - X["clr_But"],
                "X2": X["clr_Pr"] - X["clr_But"],
                "X3": X["clr_Val"] - X["clr_But"],
            }
        )
        return sm.OLS(y, Xr).fit()


def optimise_pha(gam: LinearGAM) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Maximise PHA prediction subject to clr sum = 0."""
    objective = lambda v: -gam.predict(v.reshape(1, -1))[0]
    cons = {"type": "eq", "fun": lambda v: np.sum(v)}
    res = minimize(objective, x0=np.zeros(4), bounds=[(-2, 2)] * 4, constraints=cons)
    if not res.success:
        raise RuntimeError(res.message)
    opt_clr = res.x
    opt_prop = np.exp(opt_clr) / np.sum(np.exp(opt_clr))
    return opt_clr, opt_prop, gam.predict(opt_clr.reshape(1, -1))


def calc_surface(
    xvar: str,
    yvar: str,
    model: LinearGAM,
    baseline: np.ndarray,
    xlim: tuple[float, float],
    ylim: tuple[float, float],
    grid_n: int = 60,
    total: float = 100.0,
    eps: float = 1e-6,
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Compute GAM surface over a 2-D grid in concentration space."""
    var_idx = {"Ac": 0, "Pr": 1, "Val": 2, "But": 3}
    ix, iy = var_idx[xvar], var_idx[yvar]

    Xg, Yg = np.meshgrid(np.linspace(*xlim, grid_n), np.linspace(*ylim, grid_n))
    Z = np.zeros_like(Xg)

    for i in range(grid_n):
        for j in range(grid_n):
            x_val, y_val = Xg[i, j], Yg[i, j]
            remain = total - (x_val + y_val)
            if remain < 0:
                Z[i, j] = np.nan
                continue
            conc = np.zeros(4)
            conc[ix], conc[iy] = x_val, y_val
            base_sum = baseline.sum() - (baseline[ix] + baseline[iy])
            for k in range(4):
                if k not in (ix, iy):
                    conc[k] = baseline[k] / base_sum * remain
            prop = np.maximum(conc / total, eps)
            prop /= prop.sum()
            clr = np.log(prop) - np.mean(np.log(prop))
            Z[i, j] = model.predict(clr.reshape(1, -1))[0]
    return Xg, Yg, Z


# =======================================================================
# 2 · Main analysis pipeline
# =======================================================================


def main(data_path: Path, output_dir: Path = Path(".")) -> None:
    """
    Run full statistical workflow.
    Args:
        data_path: path to the Excel file containing raw measurements.
        output_dir: directory for all outputs (figures, tables).
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # -------------------------------------------------------------------
    # 2.1 · Load data
    df = pd.read_excel(data_path, engine="openpyxl")
    if "HB:HV_prod" in df.columns:
        df = df.rename(columns={"HB:HV_prod": "HB_HV_prod"})

    vfa_cols = ["Substrate_Ac", "Substrate_Pr", "Substrate_Val", "Substrate_But"]
    resp_cols = ["PHA_prod", "HB_HV_prod"]

    # -------------------------------------------------------------------
    # 2.2 · CLR transform
    clr_df = clr_transform(df[vfa_cols])
    clr_df.columns = ["clr_Ac", "clr_Pr", "clr_Val", "clr_But"]
    data_clr = pd.concat([clr_df, df[resp_cols]], axis=1)

    # -------------------------------------------------------------------
    # 2.3 · Scatter-matrix with custom ranges
    ranges = {
        "clr_Ac": (-0.5, 1.5),
        "clr_Pr": (-1, 1),
        "clr_Val": (-0.5, 1),
        "clr_But": (-0.5, 1),
        "PHA_prod": (-20, 40),
        "HB_HV_prod": (0.25, 3.25),
    }
    pair = sns.pairplot(data_clr, diag_kind="hist")
    for i, var_y in enumerate(data_clr.columns):
        for j, var_x in enumerate(data_clr.columns):
            ax = pair.axes[i, j]
            ax.set_xlim(ranges[var_x])
            ax.set_ylim(ranges[var_y])
    pair.fig.savefig(output_dir / "scatter_matrix.pdf", bbox_inches="tight")

    # -------------------------------------------------------------------
    # 2.4 · Correlation matrix
    corr = data_clr.corr()
    corr.to_csv(output_dir / "correlation_matrix.csv")

    # -------------------------------------------------------------------
    # 2.5 · OLS models
    X = data_clr[["clr_Ac", "clr_Pr", "clr_Val", "clr_But"]]
    ols_pha = fit_ols_constrained(X, data_clr["PHA_prod"])
    ols_hbhv = fit_ols_constrained(X, data_clr["HB_HV_prod"])

    with open(output_dir / "ols_pha.tex", "w") as f:
        f.write(ols_pha.summary().as_latex())
    with open(output_dir / "ols_hbhv.tex", "w") as f:
        f.write(ols_hbhv.summary().as_latex())

    # -------------------------------------------------------------------
    # 2.6 · GAM models with λ grid-search
    lam_grid = np.logspace(-3, 1, 50)
    X_mat = X.to_numpy()

    gam_pha = LinearGAM(s(0) + s(1) + s(2) + s(3), fit_intercept=False)
    gam_pha.gridsearch(X_mat, data_clr["PHA_prod"].to_numpy(), lam=lam_grid)

    gam_hbhv = LinearGAM(s(0) + s(1) + s(2) + s(3), fit_intercept=False)
    gam_hbhv.gridsearch(X_mat, data_clr["HB_HV_prod"].to_numpy(), lam=lam_grid)

    # Save GAM summaries
    with open(output_dir / "gam_pha.txt", "w") as f:
        with contextlib.redirect_stdout(f):
            gam_pha.summary()
    with open(output_dir / "gam_hbhv.txt", "w") as f:
        with contextlib.redirect_stdout(f):
            gam_hbhv.summary()

    # -------------------------------------------------------------------
    # 2.7 · Partial dependence plots (2×4)
    fig_pd, axs_pd = plt.subplots(2, 4, figsize=(12, 5))
    for i in range(4):
        Xg = gam_pha.generate_X_grid(term=i)
        axs_pd[0, i].plot(Xg[:, i], gam_pha.partial_dependence(term=i, X=Xg))
        axs_pd[0, i].set_title(f"PHA – s(clr_{['Ac','Pr','Val','But'][i]})")
        Xg2 = gam_hbhv.generate_X_grid(term=i)
        axs_pd[1, i].plot(Xg2[:, i], gam_hbhv.partial_dependence(term=i, X=Xg2))
        axs_pd[1, i].set_title(f"HBHV – s(clr_{['Ac','Pr','Val','But'][i]})")
    fig_pd.tight_layout()
    fig_pd.savefig(output_dir / "partial_dependence.pdf")

    # -------------------------------------------------------------------
    # 2.8 · Optimisation for maximal PHA
    opt_clr, opt_prop, _ = optimise_pha(gam_pha)
    opt_conc = opt_prop * 100  # assuming total = 100

    # -------------------------------------------------------------------
    # 2.9 · 3-D surfaces (3×2) with fixed z-ticks and export
    baseline = opt_conc
    surf_specs = [
        ("Ac", "Pr", gam_pha, (14, 18), (9, 13), "PHBV$_{prod}$"),
        ("Ac", "Pr", gam_hbhv, (14, 18), (9, 13), "HB:HV$_{prod}$"),
        ("Pr", "Val", gam_pha, (9, 13), (11, 15), "PHBV$_{prod}$"),
        ("Pr", "Val", gam_hbhv, (9, 13), (11, 15), "HB:HV$_{prod}$"),
        ("But", "Val", gam_pha, (57, 61), (11, 15), "PHBV$_{prod}$"),
        ("But", "Val", gam_hbhv, (57, 61), (11, 15), "HB:HV$_{prod}$"),
    ]
    zticks_left = np.array([15, 20, 25, 30, 35])
    zticks_right = np.array([1.4, 1.7, 2.0, 2.3, 2.6])

    fig3d = plt.figure(figsize=(5.5, 7))  # inches
    for idx, (xv, yv, mod, xlim, ylim, zlab) in enumerate(surf_specs):
        ax = fig3d.add_subplot(3, 2, idx + 1, projection="3d")
        Xg, Yg, Z = calc_surface(xv, yv, mod, baseline, xlim, ylim)
        ax.plot_surface(Xg, Yg, Z, cmap="viridis", edgecolor="none", alpha=0.9)
        ax.set_xlabel(f"{xv} (%)")
        ax.set_ylabel(f"{yv} (%)")
        ax.set_title(chr(65 + idx), loc="left", fontweight="bold")
        if idx % 2 == 0:
            ax.set_zticks(zticks_left)
            ax.set_zlim(zticks_left.min(), zticks_left.max())
        else:
            ax.set_zticks(zticks_right)
            ax.set_zlim(zticks_right.min(), zticks_right.max())
        ax.zaxis.set_major_formatter(mticker.FormatStrFormatter("%.1f"))
        ax.set_zlabel(zlab)
        ax.view_init(elev=28, azim=135)
    fig3d.tight_layout()
    fig3d.savefig(output_dir / "surface_plots.pdf")
    fig3d.savefig(output_dir / "surface_plots.png", dpi=500)

    # -------------------------------------------------------------------
    # 2.10 · Sensitivity analysis and ±10 % profiles
    # (Omitted here for brevity; replicate your earlier code inside helper
    #  functions if you wish to include these figures in the publication.)

    # -------------------------------------------------------------------
    # 2.11 · Predictions for original data and export to Excel
    preds_pha = []
    preds_hbhv = []
    for _, row in df.iterrows():
        vfa_vec = row[vfa_cols].to_numpy(dtype=float)
        clr_vec = single_clr(vfa_vec)
        preds_pha.append(gam_pha.predict(clr_vec.reshape(1, -1))[0])
        preds_hbhv.append(gam_hbhv.predict(clr_vec.reshape(1, -1))[0])

    df["Predicted_PHA"] = preds_pha
    df["Predicted_HB_HV"] = preds_hbhv

    with pd.ExcelWriter(output_dir / "results.xlsx", engine="openpyxl") as xl:
        corr.to_excel(xl, sheet_name="Correlation_Matrix")
        df.to_excel(xl, sheet_name="Predictions", index=False)

    print("Pipeline completed successfully → see outputs in", output_dir)


# =======================================================================
# 3 · Entry point
# =======================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run VFA compositional workflow.")
    parser.add_argument("excel_file", type=str, help="Path to Excel file with raw data.")
    parser.add_argument(
        "--out",
        type=str,
        default="analysis_output",
        help="Directory for figures and tables.",
    )
    args = parser.parse_args()

    main(Path(args.excel_file), output_dir=Path(args.out))

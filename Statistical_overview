#!/usr/bin/env python
# coding: utf-8
"""
Statistical analysis of multi-parameter experiments:
  • five-number summary & Shapiro–Wilk
  • Pearson & partial correlations
  • PCA (scores + loadings)
  • Custom box-plots

Author      : <Your Name>
Created     : 2025-07-10
Reproducible: SEED = 42
Python      : ≥3.9
"""

# ======================================================================
# 0 · Imports and global settings
# ======================================================================

from __future__ import annotations

import argparse
import io
import random
import re
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.ticker as mticker
from matplotlib.patches import FancyArrowPatch
from scipy.stats import shapiro
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import statsmodels.stats.api as sms

# ── reproducibility ───────────────────────────────────────────────────
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

plt.rcParams.update(
    {
        "font.family": "serif",
        "axes.labelsize": 11,
        "axes.titlesize": 12,
        "figure.dpi": 150,
    }
)

# ======================================================================
# 1 · Helper functions
# ======================================================================


def format_param(name: str) -> str:
    """Convert '_' → subscripts, '^' → superscripts for LaTeX labels."""
    name = re.sub(r"\^(?!{)(\S+)", r"^{\1}", name)
    name = re.sub(r"_(?!{)(\S+)", r"_{\1}", name)
    return f"${name}$"


def five_number_summary(x: pd.Series) -> pd.Series:
    """Return count, min, Q1, median, Q3, max."""
    return pd.Series(
        {
            "Count": x.count(),
            "Min": x.min(),
            "Q1": x.quantile(0.25),
            "Median": x.median(),
            "Q3": x.quantile(0.75),
            "Max": x.max(),
        }
    )


def partial_corr_matrix(df_in: pd.DataFrame) -> pd.DataFrame:
    """Partial correlation via precision matrix (inverse of correlation)."""
    corr = df_in.corr().to_numpy()
    prec = np.linalg.pinv(corr)
    pmat = -prec / np.sqrt(np.outer(np.diag(prec), np.diag(prec)))
    np.fill_diagonal(pmat, 1.0)
    return pd.DataFrame(pmat, index=df_in.columns, columns=df_in.columns)


# ======================================================================
# 2 · Core analysis
# ======================================================================


def load_data(path: Path) -> pd.DataFrame:
    """Read Excel or CSV with first two rows as header (multi-index)."""
    if path.suffix.lower() == ".xlsx":
        return pd.read_excel(path, header=[0, 1])
    elif path.suffix.lower() == ".csv":
        return pd.read_csv(path, header=[0, 1])
    else:
        raise ValueError("Unsupported file format; use .xlsx or .csv")


def tidy_dataframe(df: pd.DataFrame) -> tuple[pd.DataFrame, list[str]]:
    """
    • Harmonise column names for identifier columns.
    • Melt to long format with columns:
        Parameter, Shapiro-Wilk, Boxplot, PCA, range, Gruppo, Titolo,
        Test, Replicate, Value
    Returns
    -------
    df_long : pd.DataFrame
    id_vars : list[str]
    """
    id_map = {
        "Parameter": "Parameter",
        "Shapiro-Wilk": "Shapiro-Wilk",
        "Boxplot": "Boxplot",
        "PCA": "PCA",
        "range": "range",
        "Gruppo": "Gruppo",
        "Titolo": "Titolo",
    }

    new_cols = []
    for col in df.columns:
        if isinstance(col, tuple):
            lbl = id_map.get(col[0])
            new_cols.append(lbl or col if not pd.isna(col[1]) else col[0])
        else:
            new_cols.append(col)
    df.columns = new_cols

    id_vars = list(id_map.values())
    value_vars = [c for c in df.columns if c not in id_vars]

    df_long = pd.melt(
        df,
        id_vars=id_vars,
        value_vars=value_vars,
        var_name="Test_Rep",
        value_name="Value",
    )
    df_long["Test"] = df_long["Test_Rep"].apply(
        lambda x: x[0] if isinstance(x, tuple) else x
    )
    df_long["Replicate"] = df_long["Test_Rep"].apply(
        lambda x: x[1] if isinstance(x, tuple) else None
    )
    df_long.drop(columns=["Test_Rep"], inplace=True)
    df_long["Value"] = pd.to_numeric(df_long["Value"], errors="coerce")

    return df_long, id_vars


def shapiro_by_group(df_long: pd.DataFrame) -> pd.DataFrame:
    """Run Shapiro-Wilk where the flag column == 1."""
    def _run(group):
        flag = pd.to_numeric(group["Shapiro-Wilk"].iloc[0], errors="coerce")
        vals = group["Value"].dropna()
        if len(vals) < 3 or flag != 1:
            return pd.Series(
                {"W": np.nan, "p_value": np.nan, "Normality": "Not tested"}
            )
        stat, p = shapiro(vals)
        return pd.Series(
            {
                "W": stat,
                "p_value": p,
                "Normality": "Normal" if p > 0.05 else "Not normal",
            }
        )

    return (
        df_long.groupby(["Parameter", "Test"], group_keys=False)
        .apply(_run)
        .reset_index()
    )


def pca_analysis(df_wide: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:
    """Standardise, fit PCA (2 components), return loadings, scores, variance."""
    scaler = StandardScaler()
    X = scaler.fit_transform(df_wide.dropna())
    pca = PCA(n_components=2, random_state=SEED)
    scores = pca.fit_transform(X)
    loadings = pca.components_.T
    var = pca.explained_variance_ratio_
    load_df = (
        pd.DataFrame(loadings, columns=["PC1", "PC2"], index=df_wide.columns)
        .reset_index()
        .rename(columns={"index": "Parameter"})
    )
    scor_df = (
        pd.DataFrame(scores, columns=["PC1", "PC2"], index=df_wide.dropna().index)
        .reset_index()
    )
    return load_df, scor_df, var


# ======================================================================
# 3 · Plotting functions
# ======================================================================


def plot_heatmap(ax, matrix: pd.DataFrame, title: str) -> None:
    sns.heatmap(matrix, ax=ax, annot=True, cmap="coolwarm", vmin=-1, vmax=1, fmt=".2f")
    ax.set_title(title, loc="left", fontsize=16, fontweight="bold")
    ax.set_xlabel("")
    ax.set_xticklabels(
        [format_param(t.get_text()) for t in ax.get_xticklabels()],
        rotation=45,
        ha="right",
    )
    ax.set_yticklabels(
        [format_param(t.get_text()) for t in ax.get_yticklabels()], rotation=0
    )
    for spine in ax.spines.values():
        spine.set_edgecolor("black")


def plot_pca_biplot(
    ax, scores: pd.DataFrame, loads: pd.DataFrame, var: np.ndarray
) -> None:
    sns.scatterplot(
        data=scores, x="PC1", y="PC2", hue="Test", palette="colorblind", s=90, ax=ax
    )
    scale = 8
    for _, row in loads.iterrows():
        x, y = row["PC1"] * scale, row["PC2"] * scale
        arrow = FancyArrowPatch(
            (0, 0), (x, y), arrowstyle="-|>", mutation_scale=18, color="black", lw=1.5
        )
        ax.add_patch(arrow)
        ax.text(x * 1.15, y * 1.15, format_param(row["Parameter"]), fontsize=10)

    ax.set_xlabel(f"PC1 ({var[0]*100:.1f} %)")
    ax.set_ylabel(f"PC2 ({var[1]*100:.1f} %)")
    ax.set_xlim(-5, 5)
    ax.set_ylim(-5, 5)
    ax.set_title("(B) PCA Biplot", loc="left", fontsize=16, fontweight="bold")
    for spine in ax.spines.values():
        spine.set_edgecolor("black")


def plot_boxplots(df_box: pd.DataFrame, out_dir: Path) -> None:
    groups = ["Input", "Process"]
    ncols, nrows = 2, 4
    desired_order = {
        "Input": ["A", "B", "C", "D"],
        "Process": ["E", "F", "G", "H"],
    }
    fig, axes = plt.subplots(nrows, ncols, figsize=(9, 12), squeeze=False)
    for c, grp in enumerate(groups):
        axes[0, c].set_title(grp, fontsize=14, fontweight="bold", pad=15)
        subset = df_box[df_box["Gruppo"] == grp]
        params = (
            subset[["Parameter", "Titolo"]]
            .drop_duplicates()
            .assign(
                Titolo=lambda d: pd.Categorical(
                    d["Titolo"], ordered=True, categories=desired_order[grp]
                )
            )
            .sort_values("Titolo")
        )
        for r in range(nrows):
            ax = axes[r, c]
            if r < len(params):
                p, letter = params.iloc[r]
                data = subset[subset["Parameter"] == p]
                sns.boxplot(x="Test", y="Value", data=data, palette="colorblind", ax=ax)
                if ax.get_legend():
                    ax.legend_.remove()
                ax.set_xlabel("")
                ax.set_ylabel(format_param(p))
                rng = data["range"].iloc[0]
                if isinstance(rng, str) and "," in rng:
                    low, high = map(float, rng.split(","))
                    ax.set_ylim(low, high)
                ax.text(
                    0.01,
                    1.05,
                    f"({letter})",
                    transform=ax.transAxes,
                    fontweight="bold",
                )
            else:
                ax.axis("off")
    fig.tight_layout()
    fig.savefig(out_dir / "boxplots.pdf")


# ======================================================================
# 4 · Main pipeline
# ======================================================================


def main(data_file: Path, out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)

    # 4.1 · Load & tidy
    raw_df = load_data(data_file)
    df_long, id_vars = tidy_dataframe(raw_df)

    # 4.2 · Five-number summary
    summary = (
        df_long.groupby(["Parameter", "Test"])["Value"]
        .apply(five_number_summary)
        .reset_index()
    )

    # 4.3 · Shapiro–Wilk
    shapiro_df = shapiro_by_group(df_long)

    # 4.4 · Correlations & PCA (only parameters with PCA == 1)
    pca_params = df_long[df_long["PCA"] == 1]["Parameter"].unique()
    df_wide = (
        df_long.pivot_table(
            index=["Test", "Replicate"], columns="Parameter", values="Value"
        )[pca_params]
    )

    pearson = df_wide.corr(method="pearson")
    partial = partial_corr_matrix(df_wide)

    load_df, score_df, var_exp = pca_analysis(df_wide)

    # 4.5 · Save tables
    with pd.ExcelWriter(out_dir / "stat_results.xlsx") as xl:
        summary.to_excel(xl, sheet_name="Summary", index=False)
        shapiro_df.to_excel(xl, sheet_name="Shapiro-Wilk", index=False)
        pearson.to_excel(xl, sheet_name="Pearson")
        partial.to_excel(xl, sheet_name="PartialCorr")
        load_df.to_excel(xl, sheet_name="PCA_Loadings", index=False)
        score_df.to_excel(xl, sheet_name="PCA_Scores", index=False)

    pearson.to_csv(out_dir / "pearson_corr.csv")
    partial.to_csv(out_dir / "partial_corr.csv")

    # 4.6 · Unified figure (Pearson, PCA, Partial)
    fig, axes = plt.subplots(3, 1, figsize=(10, 20))
    plot_heatmap(axes[0], pearson, "(A) Pearson Correlation")
    plot_pca_biplot(axes[1], score_df, load_df, var_exp)
    plot_heatmap(axes[2], partial, "(C) Partial Correlation")
    fig.tight_layout()
    fig.savefig(out_dir / "summary_plots.pdf")
    fig.savefig(out_dir / "summary_plots.png", dpi=300)

    # 4.7 · Box-plots for flagged parameters
    df_box = df_long[df_long["Boxplot"].astype(float) == 1]
    if not df_box.empty:
        plot_boxplots(df_box, out_dir)

    # 4.8 · Console report (normality)
    print("\nNormality outcomes:")
    for _, row in shapiro_df.iterrows():
        p, t = row["Parameter"], row["Test"]
        if pd.isna(row["W"]):
            msg = f"{p} – {t}: {row['Normality']}"
        else:
            msg = f"{p} – {t}: W={row['W']:.3f}, p={row['p_value']:.3f} → {row['Normality']}"
        print(msg)

    print(f"\n✔ Analysis complete — outputs saved in “{out_dir}”")


# ======================================================================
# 5 · CLI
# ======================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Multivariate statistical workflow")
    parser.add_argument("data_file", type=str, help="Path to Excel/CSV data file")
    parser.add_argument("--out", type=str, default="analysis_output", help="Output dir")
    args = parser.parse_args()

    main(Path(args.data_file), Path(args.out))
